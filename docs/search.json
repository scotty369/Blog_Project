[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Scott Townsend",
    "section": "",
    "text": "About me:\nHi there! My name is Scott Townsend, and I’m a senior majoring in Data Science at Brigham Young University - Idaho. Currently, I’m working on my Senior Data Science Project, which has been an exciting and challenging journey.\nThrough this blog, I’ll be sharing updates on my project as I progress, along with any insights and challenges I face along the way. I’ll also showcase other data science projects I’ve completed, as well as those I’m actively working on, ranging from statistical analyses to machine learning applications."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/Week_01/index.html",
    "href": "posts/Week_01/index.html",
    "title": "Week 1",
    "section": "",
    "text": "As part of my Senior Data Science Project, I’m creating a tool that automatically generates captions for images. This tool aims to help visually impaired individuals understand pictures and streamline tasks like photo tagging and social media captioning.\nThe model will be trained on the Flickr8k dataset, which contains thousands of images paired with captions. The goal is to build a model that can analyze an image and generate a relevant description using machine learning techniques.\n\n\n\nThe following deliverables are part of this project:\n\nCleaned Data: Organize the dataset by ensuring that images and captions are properly matched.\nWorking Model: A model that extracts features from images (CNN) and generates captions (LSTM/Transformer).\nPerformance Check: Evaluate how well the captions align with the images.\nVisuals: Graphs and diagrams showing the model’s performance and where it focuses in the image.\nWrite-Up: A brief explanation of the project and its potential applications.\n\n\n\n\nFirst, I load the dataset and ensure that the images and captions are properly matched:\nimport pandas as pd\nimport os\n\nimage_path = '/Users/scotttow123/Documents/BYUI/Senior_Project/Images'\ncaption_file = '/Users/scotttow123/Documents/BYUI/Senior_Project/captions.txt'\n\ntry:\n    data = pd.read_csv(caption_file)\n    print(\"Data loaded successfully\")\nexcept FileNotFoundError:\n    print(f\"Error: The file {caption_file} was not found.\")\n    data = pd.DataFrame()\n\ndata.head()\nNext, I remove any duplicates and handle missing captions:\n# Remove duplicates\nif data.duplicated().any():\n    print(f\"Found {data.duplicated().sum()} duplicate rows. Removing duplicates...\")\n    data = data.drop_duplicates()\n\n# Handle missing captions\nif data['caption'].isnull().any():\n    print(f\"Found {data['caption'].isnull().sum()} missing captions. Replacing with 'No caption'.\")\n    data['caption'] = data['caption'].fillna(\"No caption\")\n\n# Validate image paths\nvalid_image_paths = data['image'].apply(lambda x: os.path.exists(os.path.join(image_path, x)))\nif not valid_image_paths.all():\n    print(f\"Found {(~valid_image_paths).sum()} invalid image paths. Removing these rows...\")\n    data = data[valid_image_paths]\nThe code above ran successfully, indicating:\nFound 10 duplicate rows. Removing duplicates...\nThis confirmed that there were duplicates within the Flickr dataset.\nTo visualize a sample of images with their captions, I used:\ndisplay_images(data.sample(15))\nThis line of code shows a random sample of 15 images with their respective captions.\n\n\n\nRandom Sample of Images\n\n\n\n\n\nFinally, I implemented text preprocessing to clean and format the captions:\ndef text_preprocessing(data):\n    data['caption'] = data['caption'].apply(lambda x: x.lower())\n    data['caption'] = data['caption'].apply(lambda x: x.replace(\"[^A-Za-z]\", \"\"))\n    data['caption'] = data['caption'].apply(lambda x: x.replace(\"\\s+\", \" \"))\n    data['caption'] = data['caption'].apply(lambda x: \" \".join([word for word in x.split() if len(word) &gt; 1]))\n    data['caption'] = \"startseq \" + data['caption'] + \" endseq\"\n    return data\n\ndata = text_preprocessing(data)\ncaptions = data['caption'].tolist()\ncaptions[:10]\n['startseq child in pink dress is climbing up set of stairs in an entry way endseq',\n 'startseq girl going into wooden building endseq',\n 'startseq little girl climbing into wooden playhouse endseq',\n 'startseq little girl climbing the stairs to her playhouse endseq',\n 'startseq little girl in pink dress going into wooden cabin endseq',\n 'startseq black dog and spotted dog are fighting endseq',\n 'startseq black dog and tri-colored dog playing with each other on the road endseq',\n 'startseq black dog and white dog with brown spots are staring at each other in the street endseq',\n 'startseq two dogs of different breeds looking at each other on the road endseq',\n 'startseq two dogs on pavement moving toward each other endseq']\nThis code ensures that captions are converted to lowercase, unwanted characters are removed, and sequences are formatted with start and end tokens. Above we can see a snippet of what this looks like.\n\nWith the data cleaned and preprocessed, I am now ready to move on to building and training the image captioning model. More updates to come!"
  },
  {
    "objectID": "posts/Week_01/index.html#introduction",
    "href": "posts/Week_01/index.html#introduction",
    "title": "Week 1",
    "section": "",
    "text": "As part of my Senior Data Science Project, I’m creating a tool that automatically generates captions for images. This tool aims to help visually impaired individuals understand pictures and streamline tasks like photo tagging and social media captioning.\nThe model will be trained on the Flickr8k dataset, which contains thousands of images paired with captions. The goal is to build a model that can analyze an image and generate a relevant description using machine learning techniques."
  },
  {
    "objectID": "posts/Week_01/index.html#project-deliverables",
    "href": "posts/Week_01/index.html#project-deliverables",
    "title": "Week 1",
    "section": "",
    "text": "The following deliverables are part of this project:\n\nCleaned Data: Organize the dataset by ensuring that images and captions are properly matched.\nWorking Model: A model that extracts features from images (CNN) and generates captions (LSTM/Transformer).\nPerformance Check: Evaluate how well the captions align with the images.\nVisuals: Graphs and diagrams showing the model’s performance and where it focuses in the image.\nWrite-Up: A brief explanation of the project and its potential applications."
  },
  {
    "objectID": "posts/Week_01/index.html#code-example-data-loading-and-preprocessing",
    "href": "posts/Week_01/index.html#code-example-data-loading-and-preprocessing",
    "title": "Week 1",
    "section": "",
    "text": "First, I load the dataset and ensure that the images and captions are properly matched:\nimport pandas as pd\nimport os\n\nimage_path = '/Users/scotttow123/Documents/BYUI/Senior_Project/Images'\ncaption_file = '/Users/scotttow123/Documents/BYUI/Senior_Project/captions.txt'\n\ntry:\n    data = pd.read_csv(caption_file)\n    print(\"Data loaded successfully\")\nexcept FileNotFoundError:\n    print(f\"Error: The file {caption_file} was not found.\")\n    data = pd.DataFrame()\n\ndata.head()\nNext, I remove any duplicates and handle missing captions:\n# Remove duplicates\nif data.duplicated().any():\n    print(f\"Found {data.duplicated().sum()} duplicate rows. Removing duplicates...\")\n    data = data.drop_duplicates()\n\n# Handle missing captions\nif data['caption'].isnull().any():\n    print(f\"Found {data['caption'].isnull().sum()} missing captions. Replacing with 'No caption'.\")\n    data['caption'] = data['caption'].fillna(\"No caption\")\n\n# Validate image paths\nvalid_image_paths = data['image'].apply(lambda x: os.path.exists(os.path.join(image_path, x)))\nif not valid_image_paths.all():\n    print(f\"Found {(~valid_image_paths).sum()} invalid image paths. Removing these rows...\")\n    data = data[valid_image_paths]\nThe code above ran successfully, indicating:\nFound 10 duplicate rows. Removing duplicates...\nThis confirmed that there were duplicates within the Flickr dataset.\nTo visualize a sample of images with their captions, I used:\ndisplay_images(data.sample(15))\nThis line of code shows a random sample of 15 images with their respective captions.\n\n\n\nRandom Sample of Images"
  },
  {
    "objectID": "posts/Week_03/index.html",
    "href": "posts/Week_03/index.html",
    "title": "Week 3",
    "section": "",
    "text": "Word Cloud\nBelow is a visualization of the most popular words from the caption dataset. Notably, “Man” and “Woman” emerge as the most frequently mentioned words within the captions.\n\n\n\nWord Cloud\n\n\n\n\nCaption Model Architecture\nBelow is the code for defining and compiling a neural network model designed for image captioning. The model combines image and text features, processes them through LSTM layers, and generates captions.\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Reshape, Embedding, LSTM, Dropout, add, concatenate\nfrom keras.utils import plot_model\n\n# Define input layers\ninput1 = Input(shape=(1920,))\ninput2 = Input(shape=(max_length,))\n\n# Image feature layers\nimg_features = Dense(256, activation='relu')(input1)\nimg_features_reshaped = Reshape((1, 256), input_shape=(256,))(img_features)\n\n# Text feature layers\nsentence_features = Embedding(vocab_size, 256, mask_zero=False)(input2)\nmerged = concatenate([img_features_reshaped, sentence_features], axis=1)\nsentence_features = LSTM(256)(merged)\nx = Dropout(0.5)(sentence_features)\nx = add([x, img_features])\nx = Dense(128, activation='relu')(x)\nx = Dropout(0.5)(x)\noutput = Dense(vocab_size, activation='softmax')(x)\n\n# Compile the model\ncaption_model = Model(inputs=[input1, input2], outputs=output)\ncaption_model.compile(loss='categorical_crossentropy', optimizer='adam')\n\n\nModel Visualization\n# Plot model architecture\nplot_model(caption_model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n\n# Model summary\ncaption_model.summary()\n\n\nData Generators\nThe following code sets up custom data generators for training and validation. These generators load image-caption pairs from the dataset and prepare batches for model training.\ntrain_generator = CustomDataGenerator(\n    df=train,\n    X_col='image',\n    y_col='caption',\n    batch_size=64,\n    directory=image_path,\n    tokenizer=tokenizer,\n    vocab_size=vocab_size,\n    max_length=max_length,\n    features=features\n)\n\nvalidation_generator = CustomDataGenerator(\n    df=test,\n    X_col='image',\n    y_col='caption',\n    batch_size=64,\n    directory=image_path,\n    tokenizer=tokenizer,\n    vocab_size=vocab_size,\n    max_length=max_length,\n    features=features\n)\nStay tuned for next week where we will finish the model!"
  },
  {
    "objectID": "posts/Week_03/index.html#model-architecture",
    "href": "posts/Week_03/index.html#model-architecture",
    "title": "Week 3",
    "section": "",
    "text": "The model consists of two inputs: one for the image features and another for the tokenized caption. The image features go through a dense layer, and the caption goes through an embedding layer and an LSTM. The outputs are merged, and the model learns to predict the next word in the caption.\nfrom tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add, concatenate, Reshape\n\ninput1 = Input(shape=(1920,))  # Image features from CNN\ninput2 = Input(shape=(max_length,))  # Tokenized caption\n\nimg_features = Dense(256, activation='relu')(input1)\nimg_features_reshaped = Reshape((1, 256))(img_features)\n\nsentence_features = Embedding(vocab_size, 256)(input2)\nmerged = concatenate([img_features_reshaped, sentence_features], axis=1)\nsentence_features = LSTM(256)(merged)\nx = Dropout(0.5)(sentence_features)\nx = add([x, img_features])\nx = Dense(128, activation='relu')(x)\nx = Dropout(0.5)(x)\noutput = Dense(vocab_size, activation='softmax')(x)\n\ncaption_model = Model(inputs=[input1, input2], outputs=output)\ncaption_model.compile(loss='categorical_crossentropy', optimizer='adam')"
  },
  {
    "objectID": "posts/Week_03/index.html#training-the-model",
    "href": "posts/Week_03/index.html#training-the-model",
    "title": "Week 3",
    "section": "",
    "text": "I use the CustomDataGenerator to feed the data into the model, and the model is trained with checkpoints, early stopping, and learning rate reduction to avoid overfitting.\nhistory = caption_model.fit(\n        train_generator,\n        epochs=50,\n        validation_data=validation_generator,\n        callbacks=[checkpoint, earlystopping, learning_rate_reduction]\n)"
  },
  {
    "objectID": "posts/Week_03/index.html#final-thoughts",
    "href": "posts/Week_03/index.html#final-thoughts",
    "title": "Week 3",
    "section": "",
    "text": "Once the model is trained, I can use it to generate captions for unseen images. In the next post, I’ll show how the model performs and how it generates captions for test images.\nStay tuned for more insights into how the model works and its real-world applications!"
  },
  {
    "objectID": "posts/Week_02/index.html",
    "href": "posts/Week_02/index.html",
    "title": "Week 2",
    "section": "",
    "text": "Interesting statistics:\nTotal captions: 40445 Average caption length: 11.78 words Max caption length: 38 words Min caption length: 1 words\n\n\nTokenizing Captions\nThis section handles the preparation of textual data by converting image captions into sequences of integers. It also splits the dataset into training and validation sets, ensuring that 85% of the images are used for training.\n# Tokenize captions\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(captions)\nvocab_size = len(tokenizer.word_index) + 1\nmax_length = max(len(caption.split()) for caption in captions)\n\n# Train-test split\nimages = data['image'].unique().tolist()\nnimages = len(images)\n\nsplit_index = round(0.85 * nimages)\ntrain_images = images[:split_index]\nval_images = images[split_index:]\n\ntrain = data[data['image'].isin(train_images)]\ntest = data[data['image'].isin(val_images)]\n\ntrain.reset_index(inplace=True, drop=True)\ntest.reset_index(inplace=True, drop=True)\n\ntokenizer.texts_to_sequences([captions[1]])[0]\n\n\nImage Feature Extraction\nThis section uses the DenseNet201 model to extract meaningful image features, which are saved in a dictionary for use during model training. Each image is resized, normalized, and converted into a format compatible with the model.\n# Use DenseNet201 for image feature extraction\nmodel = DenseNet201()\nfe = Model(inputs=model.input, outputs=model.layers[-2].output)\n\nimg_size = 224\nfeatures = {}\nfor image in tqdm(data['image'].unique().tolist()):\n    img = load_img(os.path.join(image_path, image), target_size=(img_size, img_size))\n    img = img_to_array(img)\n    img = img / 255.\n    img = np.expand_dims(img, axis=0)\n    feature = fe.predict(img, verbose=0)\n    features[image] = feature\n\n\nCustom Data Generator\nThis custom data generator class facilitates efficient model training by yielding batches of data, including image features and tokenized caption sequences. It handles batch creation, shuffling, and ensures compatibility with the model’s input format.\nclass CustomDataGenerator(Sequence):\n\n    def __init__(self, df, X_col, y_col, batch_size, directory, tokenizer,\n                 vocab_size, max_length, features, shuffle=True):\n\n        self.df = df.copy()\n        self.X_col = X_col\n        self.y_col = y_col\n        self.directory = directory\n        self.batch_size = batch_size\n        self.tokenizer = tokenizer\n        self.vocab_size = vocab_size\n        self.max_length = max_length\n        self.features = features\n        self.shuffle = shuffle\n        self.n = len(self.df)\n\n    def on_epoch_end(self):\n        if self.shuffle:\n            self.df = self.df.sample(frac=1).reset_index(drop=True)\n\n    def __len__(self):\n        return self.n // self.batch_size\n\n    def __getitem__(self, index):\n\n        batch = self.df.iloc[index * self.batch_size:(index + 1) * self.batch_size, :]\n        X1, X2, y = self.__get_data(batch)\n        return (X1, X2), y\n\n    def __get_data(self, batch):\n\n        X1, X2, y = list(), list(), list()\n\n        images = batch[self.X_col].tolist()\n\n        for image in images:\n            feature = self.features[image][0]\n\n            captions = batch.loc[batch[self.X_col] == image, self.y_col].tolist()\n            for caption in captions:\n                seq = self.tokenizer.texts_to_sequences([caption])[0]\n\n                for i in range(1, len(seq)):\n                    in_seq, out_seq = seq[:i], seq[i]\n                    in_seq = pad_sequences([in_seq], maxlen=self.max_length)[0]\n                    out_seq = to_categorical([out_seq], num_classes=self.vocab_size)[0]\n                    X1.append(feature)\n                    X2.append(in_seq)\n                    y.append(out_seq)\n\n        X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n\n        return X1, X2, y\nNext week we will look at the model creation and the image and text feature layers!"
  },
  {
    "objectID": "posts/Week_02/index.html#image-preprocessing",
    "href": "posts/Week_02/index.html#image-preprocessing",
    "title": "Week 2",
    "section": "",
    "text": "For each image, I resize it and normalize the pixel values to a range between 0 and 1:\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\n\ndef readImage(path, img_size=224):\n    img = load_img(path, color_mode='rgb', target_size=(img_size, img_size))\n    img = img_to_array(img)\n    img = img / 255.  # Normalize\n    return img\n\ndef text_preprocessing(data):\n    data['caption'] = data['caption'].apply(lambda x: x.lower())\n    data['caption'] = data['caption'].apply(lambda x: x.replace(\"[^A-Za-z]\", \"\"))\n    data['caption'] = data['caption'].apply(lambda x: x.replace(\"\\s+\", \" \"))\n    data['caption'] = data['caption'].apply(lambda x: \" \".join([word for word in x.split() if len(word) &gt; 1]))\n    data['caption'] = \"startseq \" + data['caption'] + \" endseq\"\n    return data\n\ndata = text_preprocessing(data)\ncaptions = data['caption'].tolist()\ncaptions[:10]\nNext, I will tokenize the captions, splitting them into words and assigning each word a unique integer.\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(captions)\nvocab_size = len(tokenizer.word_index) + 1  # Include padding token\nmax_length = max(len(caption.split()) for caption in captions)\n\n# Split into train and test\nimages = data['image'].unique().tolist()\nsplit_index = round(0.85 * len(images))\ntrain_images = images[:split_index]\nval_images = images[split_index:]\n\ntrain = data[data['image'].isin(train_images)]\ntest = data[data['image'].isin(val_images)]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Senior Project Blog",
    "section": "",
    "text": "Week 3\n\n\n\n\n\n\nnews\n\n\ncode\n\n\n\n\n\n\n\n\n\nFeb 11, 2025\n\n\nScott Townsend\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 2\n\n\n\n\n\n\nnews\n\n\ncode\n\n\n\n\n\n\n\n\n\nFeb 4, 2025\n\n\nScott Townsend\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 1\n\n\n\n\n\n\nnews\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 27, 2025\n\n\nScott Townsend\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "Streaming Services Analysis\n\n\n\n\n\n\nPython\n\n\nDataviz\n\n\n\n\n\n\n\n\n\nFeb 18, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCrime Analysis\n\n\n\n\n\n\nR\n\n\nDataviz\n\n\n\n\n\n\n\n\n\nFeb 18, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects.html#projects",
    "href": "projects.html#projects",
    "title": "📚 Projects",
    "section": "",
    "text": "A collection of my personal projects showcasing my work in data analysis, statistical modeling, and machine learning.\n\n\n\nStreaming Services Data Analysis (January 2025 - Present)\nAnalyzed Netflix, Hulu, Disney+, and Amazon content libraries to uncover trends in movie durations, genres, and ratings.\n\nUtilized Polars and Pandas for efficient data processing.\n\nVisualized insights with Seaborn, Matplotlib, and Plotly, exploring genre popularity, rating distributions, and country-wise content production.\n\nBuilt interactive dashboards and statistical summaries to inform data-driven decisions.\n\nImage Captioning Tool for Visually Impaired Users (December 2024 - Present)\nDeveloped a deep learning model to generate descriptive captions for images, aiding visually impaired individuals and automating social media captioning.\n\nImplemented a CNN (VGG16) for image feature extraction and an LSTM/Transformer for caption generation.\n\nVisualized model attention areas using heatmaps and presented insights through data visualizations.\n\nCrime Analysis\nExploring crime trends through data analysis and visualization.\nStudent Performance Analysis\nAnalyzing factors affecting student academic performance.\nStudent Performance Predictive ML Model\nBuilding a predictive model to forecast student success based on historical data.\nPopulation Growth Analysis\nInvestigating population growth trends using statistical methods.\n\n💡 Feel free to explore my projects for detailed analysis and visualization."
  },
  {
    "objectID": "projects.html#my-current-projects",
    "href": "projects.html#my-current-projects",
    "title": "📚 Projects",
    "section": "",
    "text": "A collection of my personal projects showcasing my work in data analysis, statistical modeling, and machine learning.\n\n\n\nStreaming Services Data Analysis (January 2025 - Present)\nAnalyzed Netflix, Hulu, Disney+, and Amazon content libraries to uncover trends in movie durations, genres, and ratings.\n\nUtilized Polars and Pandas for efficient data processing.\n\nVisualized insights with Seaborn, Matplotlib, and Plotly, exploring genre popularity, rating distributions, and country-wise content production.\n\nBuilt interactive dashboards and statistical summaries to inform data-driven decisions.\n\nImage Captioning Tool for Visually Impaired Users (December 2024 - Present)\nDeveloped a deep learning model to generate descriptive captions for images, aiding visually impaired individuals and automating social media captioning.\n\nImplemented a CNN (VGG16) for image feature extraction and an LSTM/Transformer for caption generation.\n\nVisualized model attention areas using heatmaps and presented insights through data visualizations.\n\nCrime Analysis\nExploring crime trends through data analysis and visualization. Developed interactive visualizations in Python (Matplotlib, Plotly) to highlight trends in the demographics of offenders and victims.\n\nApplied data wrangling techniques to identify patterns and trends, offering actionable insights into public safety strategies.\nAnalyzed offender-victim relationships to uncover societal trends influencing crime rates.\n\nStudent Performance Analysis\nAnalyzing factors affecting student academic performance.\nStudent Performance Predictive ML Model\nBuilding a predictive model to forecast student success based on historical data.\nPopulation Growth Analysis\nInvestigating population growth trends using statistical methods.\n\n💡 Feel free to explore my projects for detailed analysis and visualization."
  },
  {
    "objectID": "projects/Crime_Analysis/index.html",
    "href": "projects/Crime_Analysis/index.html",
    "title": "Crime Analysis",
    "section": "",
    "text": "Intro"
  },
  {
    "objectID": "projects/Crime_Analysis/index.html#analysis-overview",
    "href": "projects/Crime_Analysis/index.html#analysis-overview",
    "title": "Criminal Incident Analysis",
    "section": "",
    "text": "The analysis is structured into several key sections, each exploring a different aspect of criminal incidents. Below are the main insights drawn from the data:\n\n\nThis visualization provides an overview of the frequency of criminal incidents based on their location. Understanding where crimes occur helps in targeting both public and private safety initiatives.\n\n\n\n\nResidential areas are the most common settings for criminal incidents, followed closely by public locations such as streets and sidewalks. These findings underscore the need for improved safety measures in both private homes and public spaces.\n\n\n\n\n\nThe following charts show the age distributions of both offenders and victims, offering insights into the typical age groups involved in these incidents.\n\n\n\n\n\n\n\n\n\n\n\n\n\nOffenders: The most common age group among offenders is 20–29, followed by 30–39. This indicates that younger adults are more likely to be involved in criminal activities.\nVictims: Victimization also peaks in the 20–29 age group, suggesting a strong overlap between the ages of offenders and victims. This may reflect social or environmental factors influencing both offending and victimization patterns.\n\n\n\n\n\nThe charts below compare the ethnicities of offenders and victims to reveal potential patterns of racial disparities in criminal incidents.\n\n\n\nEthnicties of the Offenders\n\n\n\n\n\n\n\n\n\n\nThe majority of both offenders and victims are categorized as non-Hispanic and non-Latino. This suggests that ethnic groups outside of the Hispanic/Latino community are more frequently involved in criminal incidents. Further analysis is needed to explore the sociocultural and systemic factors behind these patterns.\n\n\n\n\n\nThis analysis explores the racial distribution of offenders and victims, investigating whether race-related disparities are present.\n\n\n\n\n\n\n\n\n\n\n\n\n\nOffenders: Both White and African American individuals are most commonly represented among offenders, highlighting possible broader societal and economic influences.\nVictims: White individuals appear to be more frequently victimized, suggesting potential interactions between race, geographic location, and social dynamics. This warrants further investigation into how systemic factors contribute to these disparities.\n\n\n\n\n\nThis chart categorizes the types of offenses involved in the reported incidents, giving us a detailed breakdown of criminal activity.\n\n\n\n\n\n\n\n\nProperty destruction is the most prevalent offense, followed by simple assaults, breaking and entering, and drug-related crimes. This breakdown provides a clearer picture of the types of criminal activities most commonly occurring in these incidents.\n\n\n\n\n\nThe following chart provides an overview of the types of weapons used during criminal incidents, offering valuable insights into the role of weaponry in violent crime.\n\n\n\n\n\n\n\n\nPersonal weapons (hands, fists, etc.) are the most frequently used in incidents, followed by firearms and knives. The prominence of physical altercations emphasizes the need for preventive measures around conflict de-escalation, while the prevalence of firearms and knives highlights the importance of effective weapon control.\n\n\n\n\n\nThis chart examines the relationship dynamics between offenders and victims, uncovering patterns in how these relationships might influence criminal behavior.\n\n\n\n\n\n\n\n\nThe most common relationship between offenders and victims is that of strangers, where no prior connection exists between the parties. Other significant relationships include boyfriends/girlfriends, friends, spouses, and acquaintances. This indicates a range of scenarios, from random violence to incidents stemming from interpersonal conflicts."
  },
  {
    "objectID": "projects/Crime_Analysis/index.html#conclusion",
    "href": "projects/Crime_Analysis/index.html#conclusion",
    "title": "Criminal Incident Analysis",
    "section": "",
    "text": "This analysis uncovers several critical patterns in criminal activity related to gun violence. Key findings include:\n\nYounger individuals, particularly in their 20s and 30s, are more likely to be involved both as offenders and victims.\nThe most common offenses involve property destruction, assaults, and drug-related crimes.\nBoth White and African American individuals are most commonly represented among offenders, while White individuals are more frequently victimized.\nPersonal weapons and firearms are the most common weapons used in violent incidents.\n\nThese insights are crucial for understanding the social dynamics behind gun violence. By further exploring these trends and integrating additional datasets, law enforcement agencies and policymakers can make more informed decisions about crime prevention, resource allocation, and public safety initiatives."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Senior Project Blog",
    "section": "",
    "text": "Ridgelines\n\n\n\n\n\n\nPython\n\n\nDataviz\n\n\n\n\n\n\n\n\n\nFeb 23, 2025\n\n\nScott Townsend\n\n\n\n\n\n\n\n\n\n\n\n\nResume\n\n\n\n\n\n\nQuarto\n\n\nLaTeX\n\n\n\n\n\n\n\n\n\nFeb 23, 2025\n\n\nScott Townsend\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 3\n\n\n\n\n\n\nnews\n\n\ncode\n\n\n\n\n\n\n\n\n\nFeb 11, 2025\n\n\nScott Townsend\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 2\n\n\n\n\n\n\nnews\n\n\ncode\n\n\n\n\n\n\n\n\n\nFeb 4, 2025\n\n\nScott Townsend\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 1\n\n\n\n\n\n\nnews\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 27, 2025\n\n\nScott Townsend\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Week_01/index.html#text-preprocessing",
    "href": "posts/Week_01/index.html#text-preprocessing",
    "title": "Week 1",
    "section": "",
    "text": "Finally, I implemented text preprocessing to clean and format the captions:\ndef text_preprocessing(data):\n    data['caption'] = data['caption'].apply(lambda x: x.lower())\n    data['caption'] = data['caption'].apply(lambda x: x.replace(\"[^A-Za-z]\", \"\"))\n    data['caption'] = data['caption'].apply(lambda x: x.replace(\"\\s+\", \" \"))\n    data['caption'] = data['caption'].apply(lambda x: \" \".join([word for word in x.split() if len(word) &gt; 1]))\n    data['caption'] = \"startseq \" + data['caption'] + \" endseq\"\n    return data\n\ndata = text_preprocessing(data)\ncaptions = data['caption'].tolist()\ncaptions[:10]\n['startseq child in pink dress is climbing up set of stairs in an entry way endseq',\n 'startseq girl going into wooden building endseq',\n 'startseq little girl climbing into wooden playhouse endseq',\n 'startseq little girl climbing the stairs to her playhouse endseq',\n 'startseq little girl in pink dress going into wooden cabin endseq',\n 'startseq black dog and spotted dog are fighting endseq',\n 'startseq black dog and tri-colored dog playing with each other on the road endseq',\n 'startseq black dog and white dog with brown spots are staring at each other in the street endseq',\n 'startseq two dogs of different breeds looking at each other on the road endseq',\n 'startseq two dogs on pavement moving toward each other endseq']\nThis code ensures that captions are converted to lowercase, unwanted characters are removed, and sequences are formatted with start and end tokens. Above we can see a snippet of what this looks like.\n\nWith the data cleaned and preprocessed, I am now ready to move on to building and training the image captioning model. More updates to come!"
  },
  {
    "objectID": "projects/Streaming_Services/index.html",
    "href": "projects/Streaming_Services/index.html",
    "title": "Streaming Services Analysis",
    "section": "",
    "text": "This analysis will dive into data from four leading streaming platforms—Netflix, Hulu, Amazon Prime, and Disney+—to explore trends, viewership patterns, and key performance metrics. By examining each service’s unique offerings and subscriber behaviors, we aim to uncover insights that highlight their market positions and user preferences in the ever-evolving digital entertainment landscape.\n\n\n\n\n\n\nThis visualization illustrates the proportion of the different platforms and their count of releases throughout the years. It appears that Amazon Prime has the highest count of releases for now, with Netflix close behind.\n\n\n\n\nThis line graph tracks the growth in content offerings for each platform over time. We can see here that among the platforms Amazon Prime again has the highest release count through the years. Although, Disney+ seems to have remained steady with their releases over time.\n\n\n\n\nThis visualization highlights the release patterns of movies and TV shows over time across all platforms. This graphic displays how much more prominent TV shows has become over the years with their value increasing more than Movies have.\n\n\n\n\n\n\n\nThis plot provides a comparative analysis of the average content duration on each platform. We have Movie and TV shows being displayed, and it appears that Amazon Prime has the longest duration in both movies and tv shows, while Disney+ has the shortest.\n\n\n\n\nThis is another boxplot that examines the variation in content duration across all platforms, showing again how Amazon Prime dominates in terms of content duration with Netflix coming in second place.\n\n\n\n\nHere is one final boxplot offering a unique point of view displaying the distribution of ratings across the different platforms. It’s interesting seeing the different shapes/sizes of each box plot. Amazon Prime seems to have the widest distribution of ratings compared to the rest.\n\n\n\n\nThis chart explores the relationship between content duration and release year. Something interesting worth noting is how dominant Amazon Prime seems to be in this graphic, which makes sense in terms of our other graphics.\n\n\n\n\nHere is one additional scatter plot, showing us a more unique view of the relase year vs duration, with the rating being colored. The blue and pink seems to stand out the most among all the other colors, indicating that TV-14 and TV-Y are the most popular.\n\n\n\n\nThe analysis reveals key differences in content strategies across the four platforms, with distinct trends in content offerings, duration, and release timing. We were able to see how dominant Amazon Prime and even Netflix were across the different platforms, displaying their popularity."
  },
  {
    "objectID": "projects/Streaming_Services/index.html#analysis-overview",
    "href": "projects/Streaming_Services/index.html#analysis-overview",
    "title": "Streaming Services Analysis",
    "section": "",
    "text": "This analysis will dive into data from four leading streaming platforms—Netflix, Hulu, Amazon Prime, and Disney+—to explore trends, viewership patterns, and key performance metrics. By examining each service’s unique offerings and subscriber behaviors, we aim to uncover insights that highlight their market positions and user preferences in the ever-evolving digital entertainment landscape."
  },
  {
    "objectID": "projects/Streaming_Services/index.html#conclusion",
    "href": "projects/Streaming_Services/index.html#conclusion",
    "title": "Streaming Services Analysis",
    "section": "",
    "text": "The analysis reveals key differences in content strategies across the four platforms, with distinct trends in content offerings, duration, and release timing. We were able to see how dominant Amazon Prime and even Netflix were across the different platforms, displaying their popularity."
  },
  {
    "objectID": "posts/Ridgelines/index.html",
    "href": "posts/Ridgelines/index.html",
    "title": "Ridgelines",
    "section": "",
    "text": "Introducing Ridgeline Plots: A Visual Feast for Data\nWhat are Ridgeline Plots?\nRidgeline plots, also known as joyplots, are a visually appealing way to display the distribution of a numerical variable across multiple categories. Imagine a series of overlapping density plots, stacked vertically, creating a “ridgeline” effect. They’re excellent for comparing distributions and spotting trends across groups.\n\nWhy Use Ridgeline Plots?\n\nClarity: They provide a clear and concise way to visualize distributions, especially when comparing many categories.\nTrend Spotting: They make it easy to identify patterns and trends across groups that might be obscured in other plot types.\nVisual Appeal: Ridgeline plots are aesthetically pleasing, making them great for presentations and reports.\nSpace Efficiency: They can pack a lot of information into a relatively small space.\n\nHow Do They Work?\nAt their core, ridgeline plots are built from kernel density estimations (KDEs). KDEs are used to estimate the probability density function of a continuous variable.\n\nCalculate KDEs: For each category, a KDE is calculated.\nShift and Stack: The KDEs are shifted vertically and stacked, creating the ridgeline effect.\nOptional Overlap: The plots can overlap to create a smoother, continuous look.\n\nWhen to Use Ridgeline Plots\n\nComparing Distributions: When you want to compare the distribution of a variable across multiple groups (e.g., comparing temperatures across months, income across regions).\nTime Series Analysis: When you want to visualize how a distribution changes over time.\nExploring Categorical Data: When you want to see how a numerical variable is distributed within different categories.\n\nHow to Create Ridgeline Plots in Python\nYou can create ridgeline plots in Python using libraries like joypy or by manually calculating and plotting KDEs with seaborn and plotly or matplotlib.\nExample using plotly & seaborn:\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\nmonths = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\ndata = []\nfor i, month in enumerate(months):\n    temperatures = np.random.normal(loc=30 + i * 3, scale=5, size=100)\n    for temp in temperatures:\n        data.append([month, temp])\ndf = pd.DataFrame(data, columns=[\"Month\", \"Temperature\"])\n\nfig = go.Figure()\n\nfor i, month in enumerate(months):\n    subset = df[df[\"Month\"] == month][\"Temperature\"]\n\n    kde = sns.kdeplot(subset, bw_adjust=0.5)\n    x, y = kde.get_lines()[0].get_data()\n    plt.close() \n\n    y_shifted = y + i * 0.3 \n\n    fig.add_trace(go.Scatter(\n        x=x, y=y_shifted, mode='lines', fill='tozerox',\n        name=month, line=dict(width=1, color='black'),\n        opacity=0.7, fillcolor=f'rgba({np.random.randint(0, 256)}, {np.random.randint(0, 256)}, {np.random.randint(0, 256)}, 0.4)'\n    ))\n\n    fig.add_trace(go.Scatter(\n        x=subset, y=[i * 0.3] * len(subset), mode='markers',\n        marker=dict(size=5, opacity=0.5, color=f'rgba({np.random.randint(0, 256)}, {np.random.randint(0, 256)}, {np.random.randint(0, 256)}, 0.5)'),\n        name=month, showlegend=False\n    ))\n\nfig.update_layout(\n    title=\"\",\n    xaxis_title=\"Average temperature (F)\",\n    yaxis=dict(\n        tickmode='array',\n        tickvals=[i * 0.3 for i in range(len(months))],\n        ticktext=months,\n        zeroline=False,\n        showgrid=False,\n        showline=False,\n        ticks=''\n    ),\n    showlegend=False,\n    plot_bgcolor='white',\n    margin=dict(l=0, r=0, t=20, b=20)\n)\n\nfig.show()"
  },
  {
    "objectID": "projects/Streaming_Services/index.html#content-volume-and-trends",
    "href": "projects/Streaming_Services/index.html#content-volume-and-trends",
    "title": "Streaming Services Analysis",
    "section": "",
    "text": "This visualization illustrates the proportion of the different platforms and their count of releases throughout the years. It appears that Amazon Prime has the highest count of releases for now, with Netflix close behind.\n\n\n\n\nThis line graph tracks the growth in content offerings for each platform over time. We can see here that among the platforms Amazon Prime again has the highest release count through the years. Although, Disney+ seems to have remained steady with their releases over time.\n\n\n\n\nThis visualization highlights the release patterns of movies and TV shows over time across all platforms. This graphic displays how much more prominent TV shows has become over the years with their value increasing more than Movies have."
  },
  {
    "objectID": "projects/Streaming_Services/index.html#content-duration-analysis",
    "href": "projects/Streaming_Services/index.html#content-duration-analysis",
    "title": "Streaming Services Analysis",
    "section": "",
    "text": "This plot provides a comparative analysis of the average content duration on each platform. We have Movie and TV shows being displayed, and it appears that Amazon Prime has the longest duration in both movies and tv shows, while Disney+ has the shortest.\n\n\n\n\nThis is another boxplot that examines the variation in content duration across all platforms, showing again how Amazon Prime dominates in terms of content duration with Netflix coming in second place.\n\n\n\n\nHere is one final boxplot offering a unique point of view displaying the distribution of ratings across the different platforms. It’s interesting seeing the different shapes/sizes of each box plot. Amazon Prime seems to have the widest distribution of ratings compared to the rest.\n\n\n\n\nThis chart explores the relationship between content duration and release year. Something interesting worth noting is how dominant Amazon Prime seems to be in this graphic, which makes sense in terms of our other graphics.\n\n\n\n\nHere is one additional scatter plot, showing us a more unique view of the relase year vs duration, with the rating being colored. The blue and pink seems to stand out the most among all the other colors, indicating that TV-14 and TV-Y are the most popular."
  },
  {
    "objectID": "posts/Resume/index.html#implementation",
    "href": "posts/Resume/index.html#implementation",
    "title": "Resume",
    "section": "Implementation",
    "text": "Implementation\nThe development of this resume is somewhat complex as it implements LaTeX in Quarto to beautifully format the perfect Resume. I have been using this in VS Code, where you can easily render the work once you format or adjust the LaTeX and markdown syntax to your liking.\nAs I have been editing my resume in VS Code using this tool, I have personally found the ability to edit the document while simultaneously viewing the changes side by side to be extremely helpful. Below is a snippet of what it looks like for me as I am editing.\n\nPlease note, for now this is only able to render the LaTeX resume document into a pdf. I plan to mess around and adjust some things in the near future so you can export it as an html if thats the sort of thing that you need as well.\nAdditionally, some cool features of the exported pdf include clickable links (all text in blue are actual links) that the reader can visit, so populate those spaces with urls to your work to make yourself more discoverable.\nThe finished version can be seen below:\n\nYou can fork or clone the repository using the below link if you want to implement this somehow! All credit goes to Alex Bass or acbass49 on GitHub. I have only made minor adjustments and changes to suit my needs. I only want to make this more discoverable by others struggling to find templates or formats that they enjoy or are satisfied with.\nCopy and paste this link into an IDE terminal of your choice (VS Code preferred) and update the files so it is personalized to you.\n# Clone this repository and adjust it to your liking! \ngit clone https://github.com/acbass49/CV_Quarto.git"
  },
  {
    "objectID": "posts/Resume/index.html#introduction",
    "href": "posts/Resume/index.html#introduction",
    "title": "Resume",
    "section": "Introduction",
    "text": "Introduction\nI remember making my first resume when I was in High School about 8 years as a Word Document. I hated how difficult it was to format it to my liking. I struggled with finding good templates and constantly shifted the wording, placement, and overall style of my Resume. Recently, in my Senior Data Science Project course my professor admonished prospective Data Scientists for still using Word, when tools such as Quarto existed. This got me interested in creating a Resume that exported a Mark down document into a pdf. I was directed to several resources, where I ultimately landed on the repository below that you can customized to your liking by: Alex Bass or acbass49 on GitHub"
  }
]