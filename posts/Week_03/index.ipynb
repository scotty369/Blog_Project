{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Week 3\"\n",
        "author: \"Scott Townsend\"\n",
        "date: \"2025-01-19\"\n",
        "categories: [news, code]\n",
        "image: \"week_2.jpg\"\n",
        "---"
      ],
      "id": "e5edc07e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Building the CNN-LSTM Model for Image Captioning\n",
        "\n",
        "In this post, I’ll walk you through the architecture of the model that generates captions for images.\n",
        "\n",
        "### CNN Feature Extraction\n",
        "\n",
        "I start by using a pre-trained **DenseNet201** model for feature extraction from the images. This CNN extracts useful features from the images, which will then be used by the LSTM model to generate captions.\n"
      ],
      "id": "e0f58d1e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from tensorflow.keras.applications import DenseNet201\n",
        "from tensorflow.keras.models import Model\n",
        "from tqdm import tqdm\n",
        "\n",
        "model = DenseNet201()\n",
        "fe = Model(inputs=model.input, outputs=model.layers[-2].output)\n",
        "\n",
        "img_size = 224\n",
        "features = {}\n",
        "for image in tqdm(data['image'].unique().tolist()):\n",
        "    img = load_img(os.path.join(image_path, image), target_size=(img_size, img_size))\n",
        "    img = img_to_array(img)\n",
        "    img = img / 255.\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    feature = fe.predict(img, verbose=0)\n",
        "    features[image] = feature"
      ],
      "id": "1e737aba",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Architecture\n",
        "\n",
        "The model consists of two inputs: one for the image features and another for the tokenized caption. The image features go through a dense layer, and the caption goes through an embedding layer and an LSTM. The outputs are merged, and the model learns to predict the next word in the caption.\n"
      ],
      "id": "62e6ff64"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add, concatenate, Reshape\n",
        "\n",
        "input1 = Input(shape=(1920,))  # Image features from CNN\n",
        "input2 = Input(shape=(max_length,))  # Tokenized caption\n",
        "\n",
        "img_features = Dense(256, activation='relu')(input1)\n",
        "img_features_reshaped = Reshape((1, 256))(img_features)\n",
        "\n",
        "sentence_features = Embedding(vocab_size, 256)(input2)\n",
        "merged = concatenate([img_features_reshaped, sentence_features], axis=1)\n",
        "sentence_features = LSTM(256)(merged)\n",
        "x = Dropout(0.5)(sentence_features)\n",
        "x = add([x, img_features])\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "output = Dense(vocab_size, activation='softmax')(x)\n",
        "\n",
        "caption_model = Model(inputs=[input1, input2], outputs=output)\n",
        "caption_model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "id": "919b808b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training the Model\n",
        "\n",
        "I use the CustomDataGenerator to feed the data into the model, and the model is trained with checkpoints, early stopping, and learning rate reduction to avoid overfitting.\n"
      ],
      "id": "05ece7e7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "history = caption_model.fit(\n",
        "        train_generator,\n",
        "        epochs=50,\n",
        "        validation_data=validation_generator,\n",
        "        callbacks=[checkpoint, earlystopping, learning_rate_reduction]\n",
        ")"
      ],
      "id": "a637aa94",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final Thoughts\n",
        "\n",
        "Once the model is trained, I can use it to generate captions for unseen images. In the next post, I’ll show how the model performs and how it generates captions for test images.\n",
        "\n",
        "Stay tuned for more insights into how the model works and its real-world applications!"
      ],
      "id": "c01aba4a"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}